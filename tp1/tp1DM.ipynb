{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#import csv\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        # Charger le dataset en spécifiant le séparateur correct\n",
    "        dataset = pd.read_csv(file_path, sep=';')\n",
    "        return dataset\n",
    "    except FileNotFoundError:\n",
    "        print(\"Le fichier est introuvable.\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Erreur de parsing : {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informations about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataset_info(dataset):\n",
    "    \n",
    "    # Afficher la taille du dataset\n",
    "    print(\"Shape of the dataset:\", dataset.shape)\n",
    "    \n",
    "    # Afficher les premières lignes du dataset\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    print(dataset.head())\n",
    "    \n",
    "    # Afficher des informations générales (types de données, valeurs manquantes)\n",
    "    print(\"\\nDataset information:\")\n",
    "    dataset.info()\n",
    "    \n",
    "    # Afficher les statistiques descriptives\n",
    "    print(\"\\nDescriptive statistics:\")\n",
    "    print(dataset.describe())\n",
    "\n",
    "    print(dataset.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des tendances centrales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_central_tendencies(dataset, column):\n",
    "    \"\"\"\n",
    "    Calcule les tendances centrales (moyenne, médiane, mode) d'une colonne spécifique.\n",
    "    \"\"\"\n",
    "    # Convertir la colonne en numérique, en ignorant les erreurs (valeurs non numériques)\n",
    "    dataset[column] = pd.to_numeric(dataset[column], errors='coerce')\n",
    "    \n",
    "    # Calculer les tendances centrales\n",
    "    mean_value = dataset[column].mean()\n",
    "    median_value = dataset[column].median()\n",
    "    mode_value = dataset[column].mode()[0] if not dataset[column].mode().empty else None  # Mode peut retourner plusieurs valeurs\n",
    "    \n",
    "    return {\n",
    "        \"moyenne\": mean_value,\n",
    "        \"médiane\": median_value,\n",
    "        \"mode\": mode_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### le resume des cinques nombres (min, q1, mediane, q3, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quartiles(dataset, column):\n",
    "    \"\"\"\n",
    "    Calcule les quartiles (Q0, Q1, Q2, Q3, Q4) d'une colonne spécifique.\n",
    "    \"\"\"\n",
    "    quartiles = dataset[column].quantile([0, 0.25, 0.5, 0.75, 1])\n",
    "    return {\n",
    "        'Q0': quartiles[0.00],  # Minimum\n",
    "        'Q1': quartiles[0.25],  # Premier quartile\n",
    "        'Q2': quartiles[0.50],  # Médiane\n",
    "        'Q3': quartiles[0.75],  # Troisième quartile\n",
    "        'Q4': quartiles[1.00]   # Maximum\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  il faut calculer les quartiles sans la fonction predefinie ( en utilisant la formule de calcul)\n",
    "   - n/4 >  Q1\n",
    "   - n*3/4 >  Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropna() pour supprimer les valeurs qui sont nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_manualQuartiles(dataset, column):\n",
    "\n",
    "    # il faut tout d'abord trier le dataset\n",
    "    sorted_data = np.sort(pd.to_numeric(dataset[column].dropna().values))\n",
    "    \n",
    "    # le nombre d'instances dans le dataset\n",
    "    n = len(sorted_data)\n",
    "\n",
    "    # calculer les indices des quartiles \n",
    "\n",
    "    min_indice = 0  \n",
    "    Q1_indice = int((0.25*(n+1))-1)\n",
    "    Q2_indice = int((0.5*(n+1)-1))\n",
    "    Q3_indice = int((0.75*(n+1)-1))\n",
    "    max_indice = n - 1 \n",
    "\n",
    "    if n%4 != 0 :\n",
    "        Q1 = (sorted_data[Q1_indice]+sorted_data[Q1_indice+1])/2\n",
    "    else :\n",
    "        Q1 = sorted_data[Q1_indice]\n",
    "    \n",
    "    Q2 = (sorted_data[Q2_indice]+ sorted_data[Q2_indice +1])/2 if n%2 != 0 else sorted_data[Q2_indice]\n",
    "    Q3 = (sorted_data[Q3_indice]+ sorted_data[Q3_indice +1])/2 if 3*n%4 != 0 else sorted_data[Q3_indice]\n",
    "\n",
    "    return {\n",
    "        'min' : sorted_data[min_indice],\n",
    "        'quartile 1' : Q1,\n",
    "        'mediane': Q2,\n",
    "        'quartile3' : Q3,\n",
    "        'max' : sorted_data[max_indice]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le nombre et pourcentage de valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_missing_values(dataset, column):\n",
    "   \n",
    "    total_values = dataset[column].shape[0]\n",
    "    missing_values = dataset[column].isnull().sum()\n",
    "    percentage_missing = (missing_values / total_values) * 100\n",
    "    \n",
    "    print(f\"Nombre de valeurs manquantes dans '{column}': {missing_values}\")\n",
    "    print(f\"Pourcentage de valeurs manquantes dans '{column}': {percentage_missing:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le nombre de valeurs uniques d'une colonne donnee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_values(dataset, column):\n",
    "    \n",
    "    unique_values = dataset[column].nunique()\n",
    "    \n",
    "    print(f\"Nombre de valeurs uniques dans '{column}': {unique_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (9009, 11)\n",
      "\n",
      "First few rows of the dataset:\n",
      "                   ep (ms)                   Acc_x               Acc_y  \\\n",
      "0  2019-01-11 15:08:05.200                  0.0135               0.977   \n",
      "1  2019-01-11 15:08:05.400  -0.0014999999999999996  0.9704999999999999   \n",
      "2  2019-01-11 15:08:05.600   0.0013333333333333333  0.9716666666666667   \n",
      "3  2019-01-11 15:08:05.800                  -0.024               0.957   \n",
      "4  2019-01-11 15:08:06.000   -0.027999999999999997  0.9576666666666666   \n",
      "\n",
      "                  Acc_z          Gyro_x               Gyro_y  \\\n",
      "0                -0.071  -2.094.366.723          257.720.316   \n",
      "1  -0.07949999999999999         -16.826              -0.8904   \n",
      "2  -0.06433333333333334     526.942.212  -0.2559999999999999   \n",
      "3               -0.0735           8.061              -45.244   \n",
      "4                -0.115           2.439              -15.486   \n",
      "\n",
      "               Gyro_z ID  Label Category   Set  \n",
      "0  0.9388000000000002  B  bench    heavy  30.0  \n",
      "1              21.708  B  bench    heavy  30.0  \n",
      "2             -14.146  B  bench    heavy  30.0  \n",
      "3              -2.073  B  bench    heavy  30.0  \n",
      "4             -36.098  B  bench    heavy  30.0  \n",
      "\n",
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9009 entries, 0 to 9008\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   ep (ms)   8998 non-null   object \n",
      " 1   Acc_x     9006 non-null   object \n",
      " 2   Acc_y     9007 non-null   object \n",
      " 3   Acc_z     9006 non-null   object \n",
      " 4   Gyro_x    9008 non-null   object \n",
      " 5   Gyro_y    9009 non-null   object \n",
      " 6   Gyro_z    9008 non-null   object \n",
      " 7   ID        9007 non-null   object \n",
      " 8   Label     9009 non-null   object \n",
      " 9   Category  9005 non-null   object \n",
      " 10  Set       9003 non-null   float64\n",
      "dtypes: float64(1), object(10)\n",
      "memory usage: 774.3+ KB\n",
      "\n",
      "Descriptive statistics:\n",
      "               Set\n",
      "count  9003.000000\n",
      "mean     46.105520\n",
      "std      34.108085\n",
      "min     -10.000000\n",
      "25%      23.000000\n",
      "50%      47.000000\n",
      "75%      70.000000\n",
      "max    2000.000000\n",
      "Index(['ep (ms)', 'Acc_x', 'Acc_y', 'Acc_z', 'Gyro_x', 'Gyro_y', 'Gyro_z',\n",
      "       'ID', 'Label', 'Category', 'Set'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset\n",
    "file_path = r'C:\\\\M2\\\\DM\\\\tp\\\\DatasetExos.csv'\n",
    "dataset = load_dataset(file_path)\n",
    "\n",
    "# Si le dataset est chargé avec succès, afficher les informations de base\n",
    "if dataset is not None:\n",
    "    display_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'moyenne': 0.04590467712621936, 'médiane': 0.016, 'mode': 0.078}\n",
      "{'Q0': -0.8380000000000001, 'Q1': -0.11149999999999996, 'Q2': 0.016, 'Q3': 0.1176666666666666, 'Q4': 10.255}\n",
      "{'min': -0.8380000000000001, 'quartile 1': -0.11149999999999996, 'mediane': 0.016, 'quartile3': 0.11783333333333329, 'max': 10.255}\n"
     ]
    }
   ],
   "source": [
    "# Calcul des tendances centrales pour la colonne 'Acc_x'\n",
    "tendances_centrales = calculate_central_tendencies(dataset, 'Acc_x')\n",
    "print(tendances_centrales)\n",
    "\n",
    "# Calcul des quartiles pour la colonne Acc_x\n",
    "quartiles = calculate_quartiles(dataset, 'Acc_x')\n",
    "print(quartiles)\n",
    "\n",
    "quartiles = calculate_manualQuartiles(dataset, 'Acc_x')\n",
    "print(quartiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes dans 'Acc_x': 22\n",
      "Pourcentage de valeurs manquantes dans 'Acc_x': 0.24%\n",
      "Nombre de valeurs manquantes dans 'Acc_y': 2\n",
      "Pourcentage de valeurs manquantes dans 'Acc_y': 0.02%\n",
      "Nombre de valeurs uniques dans 'Acc_x': 2754\n",
      "Nombre de valeurs uniques dans 'Acc_y': 3794\n"
     ]
    }
   ],
   "source": [
    "# Affichage des valeurs manquantes pour la colonne 'Acc_x'\n",
    "display_missing_values(dataset, 'Acc_x')\n",
    "display_missing_values(dataset, 'Acc_y')\n",
    "\n",
    "# Affichage du nombre de valeurs uniques pour la colonne 'Acc_x'\n",
    "display_unique_values(dataset, 'Acc_x')\n",
    "display_unique_values(dataset, 'Acc_y')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
